{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, you will learn how to build your own binary classification model using logistic regression algorithm trained by gradient descent. Logistic regression classification is a type of supervised machine learning, where the algorithm relies on labeled data during training and tries to predict classes during testing. You will apply logistic regression classifier to spam detection, and you can compare this approach to the [naive bayes classifier](http://online.cambridgecoding.com/notebooks/cca_admin/using-your-own-spam-filter-in-real-life). You will use the [spambase](http://mlr.cs.umass.edu/ml/machine-learning-databases/spambase/) dataset from the UCI Machine Learning Repository where most of the attributes indicate whether a particular word or character frequently occurs in a certain type of e-mail. \n",
    "\n",
    "On the way, you will learn the theory behind the algorithm in parallel with code implementation. \n",
    "The steps you’ll cover in this post can be summarized as follows:\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/iaaxbgc4b/blog5.png\" alt=\"blog5\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring the dataset\n",
    "\n",
    "You can download the dataset from [here](http://mlr.cs.umass.edu/ml/machine-learning-databases/spambase/). You will need to change the path \"spambase/spambase.data\" to the folder where you have downloaded the dataset. You can inspect the features and the features names from [spambase.names](http://mlr.cs.umass.edu/ml/machine-learning-databases/spambase/spambase.names) which contains a description of the features extracted as well as some simple statistics over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.external import mathjax; \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the dataset\n",
    "data=pd.read_csv(\"spambase/spambase.data\", sep=\",\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure illustrates graphically the first e-mail in the dataset. For example, there are 278 capital letters and the word \"make\" does not occur in this e-mail.\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/5owkrqicb/BLOG4_N.png\" alt=\"blog4\"/>\n",
    "\n",
    "\n",
    "You can examine the dimensions of the dataset using `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "# Exmine the dimensions of the dataset\n",
    "print (data.shape)\n",
    "num_observations,num_params = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spambase dataset contains 4601 e-mails and 57 different features, with the last column storing the label. In this dataset, the label 0 is used for ham and label 1 for spam. \n",
    "\n",
    "Let's split the dataset to X (features) and y (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the dataset to X and y\n",
    "y=data.iloc[:,-1]\n",
    "X=data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at spam vs. ham frequency in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2788\n",
      "1    1813\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spambase data set consists of 4,601 e-mails, 1,813 of which are spam (39.4%). It is always advisable to check the frequency of the labels in your data because if the data is skewed towards one class, your model might not be able to predict both the majority and minority classes well. For example, if you have a dataset with 99% of the data belonging to one class (e.g., it is frequently the case in fraud detection), your model might choose the safest strategy of always predicting the majority class and will not learn anything useful. There are techniques that deal with skewed data which will be covered in one of the next blog posts. Luckily, in this dataset the class ratio is around 60:40 and you don't need to take any action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "In general, it's good to explore the data you're working with: you can take a look at one of the previous [tutorials](http://online.cambridgecoding.com/notebooks/cca_admin/eda-and-interactive-figures-with-plotly) which discusses exploratory data analysis.\n",
    "It is usually advisable to scale your data prior to applying a classification model to avoid attributes with\n",
    "greater numeric ranges dominating those with smaller numeric ranges. There are many ways of scaling, one common scaling mechanism is to compute z-score where for each\n",
    "column the values are centered around the mean and divided by their standard deviation. Alternatively you could use [scale](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing z-scores for all variables in X\n",
    "for i in X:\n",
    "    X[i] = (X[i] - X[i].mean())/X[i].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier\n",
    "\n",
    "Classification is a broad subject addressed with many different algorithms and optimization techniques. However, the underlying basic process is the same. First of all, you will need a dataset which contains the labels you are trying to predict. The next step is to extract well-defined features from the dataset. When the dataset is ready, you split it into training and test sets and build your classification model using the training set. Next step is to test the classifier on the test set and evaluate the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the Logistic regression\n",
    "\n",
    "Let's now look into how Logistic Regression works.\n",
    "\n",
    "Logistic Regression builds upon the logistic function whose values lie in the range of 0 to 1. <img class=\"alignleft size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?sigmoid(z)&space;=&space;h(z)=\\frac{1}{1&plus;e^{-z}}\" alt=\"blog1\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/fxklbk663/blog1.png\" alt=\"blog1\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot you can see that logistic function tends towards 1 as *z* gets bigger, and towards 0 as *z* gets smaller. You can build your own logistic function using the code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    result = 1.0 / (1.0 + np.e**-z)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bias feature ($x_0$) of 1 needs to be added to the data. It is common practice in logistic regression to add bias feature, which allows you to shift the entire sigmoid function curve to the right or left to fit the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.ones: return a new array of given shape, filled with ones.  \n",
    "# np.hstack: adds the array filled with ones(np.ones) to the dataframe X. \n",
    "X = pd.DataFrame(np.hstack((np.ones((X.shape[0],1)),X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Now that you have implemented logistic function let's define logistic regression more formally.\n",
    "Logistic regression measures the relationship between the categorical dependent variable (spam vs. ham) and one or more independent variables by estimating probabilities using a logistic function.\n",
    "Logistic regression can be defined as follows:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?p(y=1|\\mathbf{x}_j)=h(\\mathbf{w}^T&space;\\mathbf{x}_j)=&space;\\frac{1}{1&plus;e^{-(\\mathbf{w}^T&space;\\mathbf{x}_j)}}&space;=&space;\\frac{1}{1&plus;e^{-(w_0x_0&plus;w_1x_1&plus;w_2x_2&plus;...&plus;w_nx_n)}}\" title=\"p(y=1|\\mathbf{x}_j)=h(\\mathbf{w}^T \\mathbf{x}_j)= \\frac{1}{1+e^{-(\\mathbf{w}^T \\mathbf{x}_j)}} = \\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2x_2+...+w_nx_n)}}\" />\n",
    "\n",
    "where you are predicting how likely it is that a given sample  $\\mathbf{x}_j$ belongs to a certain class _y_. In this case  _j_ stands for one observation (e-mail), $\\mathbf{w}$ is the weights vector and $\\mathbf{x}_j$ are the features for sample *j*. Logistic regression is nothing more than taking a linear combination of $w_0x_0+w_1x_1+w_2x_2+...+w_nx_n$ and transforming it so it can only be between 0 and 1. $w_0$ is the corresponding weight value to the bias term which you added to the dataset above. Certain features, e.g frequency of the word \"money\" (*word_freq_money*), might be more important for making a decision than other features such as frequency of the word \"our\" (*word_freq_our*), and the weights vector reflects this by giving the feature *word_freq_money* a higher weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input x: predictor variables and weights. Output is probability p(y=1|x)\n",
    "def predict(x_vals,weights):\n",
    "    return sigmoid(x_vals.dot(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, `dot` product of two vectors $\\mathbf{w}$ and $\\mathbf{x}_j$ is same as  $w_0x_0+w_1x_1+w_2x_2+...+w_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression as a classifier\n",
    "\n",
    "Since $h(\\mathbf{w}^T \\mathbf{x}_j)$ outputs a probability, you can apply a threshold of 0.5 so that all the instances with the output values above this threshold will be assigned to class 1 and the ones below the threshold to class 0. \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?prediction_j=\\left\\{\\begin{matrix}&space;1,&space;&&space;if&space;\\&space;h(\\mathbf{w^T&space;x}_j)&space;\\geq&space;0.5&space;,&space;\\\\&space;0,&space;&&space;if&space;\\&space;h(\\mathbf{w^T&space;x}_j)&space;<&space;0.5&space;\\end{matrix}\\right.\" title=\"prediction_j=\\left\\{\\begin{matrix} 1, & if \\ h(\\mathbf{w^T x}_j) \\geq 0.5 , \\\\ 0, & if \\ h(\\mathbf{w^T x}_j) < 0.5 \\end{matrix}\\right.\" />\n",
    "\n",
    "The goal now is to find the values that minimize the weights ($\\mathbf{w}$) so that the model will classify the data correctly with high accuracy. \n",
    "A popular and easy-to-use technique to calculate the parameters is to minimize a model’s error (MSE) with Stochastic Gradient Descent. The [Stochastic Gradient Descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) optimization is particularly useful when the number of samples (and the number of features) is very large. Other optimization methods that are often used are [Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) and [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function: MSE\n",
    "\n",
    "The Gradient Descent is method to estimate the weights of the model in many iterations by minimizing a cost function at every step. In this case, mean square error (MSE) is chosen as a cost function:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?MSE=\\frac{1}{2m}\\sum_{i=0}^{m}(h(\\mathbf{w}^T&space;\\mathbf{x})-\\mathbf{y})^2\" title=\"MSE=\\frac{1}{2m}\\sum_{i=0}^{m}(h(\\mathbf{w}^T \\mathbf{x})-\\mathbf{y})^2\" />\n",
    "* _Cost function_ is a function where the goal is to minimize the cost value, in this case the cost value is the mean square error.\n",
    "\n",
    "Let's implement the cost function for logistic regression, which uses the predictor variables *x_vals*, the variable to be predicted *y_vals*, the current model parameters *weights* as input, and outputs the mean square error (MSE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(x_vals, y_vals, weights):\n",
    "    n = y_vals.size\n",
    "    prediction = sigmoid(x_vals.dot(weights))\n",
    "    error = prediction - y_vals\n",
    "    try:\n",
    "        cost = (1.0/(2.0*n)) * error.T.dot(error)  # if array of errors. T = transpose.\n",
    "    except AttributeError:\n",
    "        cost = (1.0/(2.0*n)) * error ** 2.0  # if single error value\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below summarizes the steps of the logistic regression classifier.\n",
    "<img class=\"alignleft size-thumbnail img-responsive\" src=\"http://s31.postimg.org/mb1d9etzv/blog3.png\" alt=\"blog3\"/>\n",
    "\n",
    "In the diagram above, step 1 is to calculate the linear combination of the features and the weights $w_0x_0+w_1x_1+w_2x_2+...+w_nx_n$. You will need to initialize the weights ($\\mathbf{w}$) in the first step, where the weights are usually set to zero. Step 2 is to put the linear combination through the sigmoid function, which will output a probability. Now before you make final prediction you want the classifier to be confident that the weight vector indicates which features have greater contribution to the predicted variable and which features are less important for predicting whether e-mail is spam or ham. Therefore, in step 4 you ask the gradient descent to update the weights of the model by minimizing the cost function (MSE). Steps 1, 2, 3 and 4 is repeated until the MSE value doesn't change (with relative tolerance) between iterations. If the difference is within an acceptable range you have reached convergence. Finally, in step 5 the weights have been optimized and you are ready to make prediction, whether an email is spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Next step is to implement the gradient descent. Gradient Descent is a very popular optimization approach since it is intuitive and easy to understand, can easily incorporate additional data and be used with parallel processing.\n",
    "\n",
    "The logistic function is characterized by being __convex__, where convexity ensures that there are no local minima. It is generally easy to minimize convex functions numerically via optimization methods such as gradient descent. Optimizing non-convex functions, on the contrary, is very hard. Such algorithms may be trapped in so-called “local” minima, which do not correspond to the true minimum value of the cost function. \n",
    "\n",
    "Image to the left shows an example of a function that is convex, and image to the right shows an example of a non-convex function.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/94koynwaj/convex.png\" alt=\"blog3\"/>\n",
    "\n",
    "\n",
    "The convex example doesn't contain any local minima. In contrast, non-convex example contains both local and global minima, where the gradient descent might get trapped in the local minima, which will not correspond to the true minimum value.\n",
    "\n",
    "There are three types of gradient descent: stochastic gradient descent, batch gradient descent and mini-batch gradient descent. The difference is how much data is used to compute the gradient of the cost function. You make a trade-off between the time it takes to update the model and how the accuracy of the parameters updates. \n",
    "\n",
    "In this post you will learn how to implement __stochastic gradient descent (SGD)__. Batch gradient descent and mini-batch gradient descent will be covered in one of the next blog posts. SGD runs through each of the data points in the training set sequentially. For each data point, the weights $\\mathbf{w}$ are updated, according to the gradient of the\n",
    "error (*MSE*) with respect to that single training example/observation only. \n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\mathbf{w}_{j&plus;1}&space;:=&space;\\mathbf{w}_j&space;-&space;\\alpha&space;(&space;h(\\mathbf{w}^T&space;\\mathbf{x})&space;^{(i)}&space;-&space;\\mathbf{y}^{(i)}&space;)\\mathbf{x}_j^{(i)}\" title=\"\\mathbf{w}_{j+1} := \\mathbf{w}_j - \\alpha ( h(\\mathbf{w}^T \\mathbf{x}) ^{(i)} - \\mathbf{y}^{(i)} )\\mathbf{x}_j^{(i)}\" />\n",
    "\n",
    "where $\\mathbf{w}_j$ are the current weights parameters, $\\alpha$ is the learning rate and  $(h(\\mathbf{w}^T \\mathbf{x}) ^{(i)} -  \\mathbf{y}^{(i)} )\\mathbf{x}_j^{(i)}$ is the equation used to minimize cost function *MSE*.     \n",
    "\n",
    "\n",
    "Let's give a real life example of how gradient descent works. Imagine, you are standing on a hill and would like to go down as quickly as possible. If you can only see a small part of the terrain around you, what will you do? Probably, the best strategy would be to look around you and find a direction where the slope of the terrain is steepest in the downhill direction. In a next step, you will scan the visible terrain around you again and go in the direction that has the most downhill slope until there is no direction in which you can move downhill. \n",
    "\n",
    "The step size you take plays a big role in whether you will move down fast or not. What might happen if you take steps that are too big is that you will overshoot the target location and you will never find your target. At the same time, if you take steps that are too small you are more likely to reach your target point but it might take you ages to go down. Your step size from the example above is the __learning rate__ which in the formula above is denoted with $\\alpha$.\n",
    "\n",
    "\n",
    "Let's implement the __stochastic gradient descent (SGD)__, which takes as input the predictor variables *x_vals*, the variable to be predicted *y_vals*, the step size *learn_rate* and the current model parameters *weights*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(x_vals, y_vals, learn_rate, method, weights):\n",
    "    preds = predict(x_vals,weights)\n",
    "    if method == 'SGD':\n",
    "        g = (preds - y_vals) * x_vals\n",
    "    weights -= learn_rate * g * (1.0 / y_vals.size)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update` function updates the weights $\\mathbf{w}$ only once, but as mentioned above the gradient descent needs to take many steps to minimize the mean square error *(MSE)*. Before you can glue all the functions together, there is one important step that needs to be introduced, that is cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with cross-validation\n",
    "To estimate the test error of your learning algorithm, you need to perform [cross-validation](http://online.cambridgecoding.com/notebooks/cca_admin/misleading-modelling-overfitting-crossvalidation-and-the-biasvariance-tradeoff). In k-fold cross-validation, you group your dataset into k-folds, where a model is trained on k-1 of the groups and tested on the remaining group. Each group is used for testing exactly once. The testing error of the classifier is estimated by the average of the performance across all $k$ folds. Now one of the main reasons why you are doing cross validation, is that, when training a model, there is a danger that you can easily overfit to the local noise in your training dataset. You can read more about cross validation in [the previous blog post](http://online.cambridgecoding.com/notebooks/cca_admin/misleading-modelling-overfitting-crossvalidation-and-the-biasvariance-tradeoff). \n",
    "\n",
    "The `cross_validate` function glues all the functions you have implemented above together. `cross_validate` starts by splitting the dataset to [k-folds](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) and initialize the training and test error (*MSE*). The *weights* need to be initialized to zero for every fold. The gradient descent runs `max_epochs` steps, where it updates the *weights* in every step. The MSE train and test error is stored for every step in every fold. The predicted score is stored in a list (`scores_and_labels`) with the true label for all emails that belong to test set for every fold. Finally, you take the average of the *MSE* training and test error over all the folds for every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "# evaluating logistic regression model with 10 fold cross validation\n",
    "def cross_validate(X, y, method='SGD', learn_rate=0.001, max_epochs=30, k_fold=10):\n",
    "    num_observations,num_params =X.shape\n",
    "    # Initialise folds and shuffle. \n",
    "    kf = KFold(num_observations, n_folds=k_fold, random_state=1,shuffle=True) \n",
    "    IndexList = list(kf)\n",
    "    scores_and_labels = []\n",
    "    train_error=[]\n",
    "    test_error=[]\n",
    "    steps=1\n",
    "    print 'learning rate: ', learn_rate\n",
    "    ######################## Cross validation k-fold ########################\n",
    "    for fold in range(k_fold):\n",
    "        weights = np.zeros(num_params) #Initialize  the weights\n",
    "        train_error.append([]) \n",
    "        test_error.append([])\n",
    "        \n",
    "        #Access first fold indexes for training and testing data.\n",
    "        train_index, test_index = IndexList[fold]\n",
    "        train_x, test_x = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        train_y, test_y = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Initial error metrics with default weights\n",
    "        train_error[fold].append(cost_function(train_x, train_y,weights))\n",
    "        test_error[fold].append(cost_function(test_x, test_y,weights))\n",
    "        counter = 0\n",
    "        while counter < max_epochs:\n",
    "            counter += 1\n",
    "            #################stochastic gradient descent ######################\n",
    "            if method == 'SGD':\n",
    "                for i in range(len(train_y)):\n",
    "                    weights = update(train_x.iloc[i], train_y.iloc[i], learn_rate, method, weights)\n",
    "            else:\n",
    "                raise AttributeError('Specify gradient descent method as SGD')\n",
    "            ################# MSE for every fold  ############################\n",
    "            train_error[fold].append(cost_function(train_x, train_y, weights))\n",
    "            test_error[fold].append(cost_function(test_x, test_y, weights))\n",
    "        \n",
    "\n",
    "        scores_and_labels += [list(i) for i in zip(predict(test_x,weights), test_y)]\n",
    "    ################# After cross validation  ###################### \n",
    "    train_error = np.mean(np.array(train_error), axis=0)\n",
    "    test_error = np.mean(np.array(test_error), axis=0)\n",
    "    scores_and_labels = np.array(scores_and_labels)\n",
    "    \n",
    "    return train_error , test_error, scores_and_labels, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can run the code by calling the `cross_validate` function. You set the learning rate of your choice, how many steps (`max_epochs`) you want the gradient descent to take and finally visualize how the training and testing error changes for every step. Since the goal of the gradient descent is to minimize the training error *MSE*, it should decrease with every step until its convergence. \n",
    "\n",
    "* Warning - Running this function could take some time -  it depends on how many folds, number of epochs and learning rates you choose. The whole script is attached below, where you can play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate:  0.1\n",
      "learning rate:  0.001\n",
      "learning rate:  0.0001\n",
      "learning rate:  1e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112f79550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "def main():\n",
    "    data=pd.read_csv(\"spambase/spambase.data\", sep=\",\", header=None)\n",
    "    \n",
    "    # Split the dataset to X and y\n",
    "    y=data.iloc[:,-1]\n",
    "    X=data.iloc[:,:-1]\n",
    "    \n",
    "    #Normalize the data\n",
    "    for i in X:\n",
    "        X[i] = (X[i] - X[i].mean())/X[i].std()\n",
    "    # Add Bias term\n",
    "    X = pd.DataFrame(np.hstack((np.ones((X.shape[0],1)), X)))\n",
    "\n",
    "    \n",
    "    # Choose learning rate\n",
    "    learn_rate=[0.1,0.001,0.0001, 0.00001]\n",
    "    # Maximum number of steps\n",
    "    n_epochs=30\n",
    "    \n",
    "    # Plotting settings\n",
    "    epochs = [i for i in range(n_epochs+1)] #Number of epochs for plotting\n",
    "    training_style = ['b-', 'r-', 'g-', 'y-']\n",
    "    test_style = ['b--', 'r--', 'g--','y--']\n",
    "    \n",
    "    # Call cross_validate function\n",
    "    for i in range(0,len(learn_rate)):\n",
    "        (train_error, test_error, scores_and_labels, weights)=cross_validate(X, y,  method='SGD', learn_rate=learn_rate[i], max_epochs=n_epochs, k_fold=5)\n",
    "        plt.plot(epochs, train_error, training_style[i], epochs, test_error, test_style[i])\n",
    "    \n",
    "    \n",
    "    plt.title('Train and Validation Error of Logistic Regression' )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    # Make the plot pretty\n",
    "    blue = mpatches.Patch(color='blue', label='Learning rate = {}'.format(learn_rate[0]))\n",
    "    red = mpatches.Patch(color='red', label='Learning rate = {}'.format(learn_rate[1]))\n",
    "    green = mpatches.Patch(color='green', label='Learning rate = {}'.format(learn_rate[2]))\n",
    "    yellow = mpatches.Patch(color='yellow', label='Learning rate = {}'.format(learn_rate[3]))\n",
    "    solid = mlines.Line2D([], [], color='black', label='Training Error')\n",
    "    dashed = mlines.Line2D([], [], color='black', linestyle='--', label='Validation Error')\n",
    "    plt.legend(handles=[blue, red, green, yellow, solid, dashed ], numpoints=1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot you can see that it matters what learning rate you choose. If `learn_rate` is too small, gradient descent can be slow, but if `learn_rate` is too large, gradient descent might overshoot the minimum. In this case `learn_rate=0.001` seems to be the best choice, since it minimizes MSE pretty quickly. On the other hand, `learn_rate=0.1` overshoots the minimum and will never find the target.  You can also assume that the classifier is not overfitting the training data since the *MSE* training error is similar to the *MSE* test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap it up, in this post we have covered how to implement logistic regression trained by stochastic gradient descent.\n",
    "The next steps would be to add [L1 or L2 regularization](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) term to reduce model overfitting and how to choose the best hyperparameters for the model, for which [grid search](http://online.cambridgecoding.com/notebooks/cca_admin/scanning-hyperspace-how-to-tune-machine-learning-models) is often used. There are other methods for evaluating the model, too: for example, you could look into creating a confusion matrix, calculating per-class accuracy, plotting an AUC or ROC curve, or numerous other evaluation metrics.\n",
    "[`Scikit-learn`](http://scikit-learn.org/stable/) also provides packages for stochastic gradient descent and logistic regression classifier and you could compare your implementation to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap of what you have learned so far:\n",
    "- The logistic regression is a convex optimization problem. There is only one global optimum for the parameters. \n",
    "- Gradient descent is a widely used optimization algorithm, which estimates the weights of the model in many iterations by minimizing a cost function at every step.\n",
    "- One of the challenges in implementing gradient descent is choosing a good learning rate $\\alpha$: if the learning rate is set too low, then gradient descent will converge very slowly; if the learning rate is too high, gradient descent may overshoot the global minima.\n",
    "- Logistic Regression builds upon the logistic function whose values lie in the range of 0 to 1 and is widely used in binary classification.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABOUT THE AUTHOR\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/lchvjpirf/Agnes_profile.png\" alt=\"Agnes Johannsdottir\"/>\n",
    "\n",
    "\n",
    "Agnes is a master student in Business Analytics at University College London. She studied Management Engineering in Iceland and worked for 2 years as an IT consultant in supply chain. Her main interests lie in using data science methods (especially machine learning) to apply in Retail and Supply Chain businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just give me the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import pandas as pd\n",
    "\n",
    "def predict(x_vals, weights):\n",
    "    return sigmoid(x_vals.dot(weights))\n",
    "\n",
    "def cost_function(x_vals, y_vals, weights):\n",
    "    n = y_vals.size\n",
    "    prediction = sigmoid(x_vals.dot(weights))\n",
    "    error = prediction - y_vals\n",
    "    try:\n",
    "        cost = (1.0/(2.0*n)) * error.T.dot(error)\n",
    "    except AttributeError:\n",
    "        cost = (1.0/(2.0*n)) * error ** 2.0\n",
    "    return cost\n",
    "\n",
    "def update(x_vals, y_vals, learn_rate, method, weights):\n",
    "    preds = predict(x_vals,weights)\n",
    "    if method == 'SGD':\n",
    "        g = (preds - y_vals) * x_vals\n",
    "    weights -= learn_rate * g * (1.0 / y_vals.size)\n",
    "    return weights\n",
    "\n",
    "def sigmoid( x):\n",
    "    result = 1.0 / (1.0 + np.e**-x)\n",
    "    return result\n",
    "\n",
    "# evaluating logistic regression model with 10 fold cross validation\n",
    "def cross_validate(X, y, method='SGD', learn_rate=0.001, max_epochs=30, k_fold=10):\n",
    "    num_observations,num_params =X.shape\n",
    "    # Initialise folds and shuffle. \n",
    "    kf = KFold(num_observations, n_folds=k_fold, random_state=1,shuffle=True) \n",
    "    IndexList = list(kf)\n",
    "    scores_and_labels = []\n",
    "    train_error=[]\n",
    "    test_error=[]\n",
    "    steps=1\n",
    "    print 'learning rate: ', learn_rate\n",
    "    ######################## Cross validation k-fold ########################\n",
    "    for fold in range(k_fold):\n",
    "        weights = np.zeros(num_params) #Initialize  the weights\n",
    "        train_error.append([]) \n",
    "        test_error.append([])\n",
    "        \n",
    "        #Access first fold indexes for training and testing data.\n",
    "        train_index, test_index = IndexList[fold]\n",
    "        train_x, test_x = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        train_y, test_y = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Initial error metrics with default weights\n",
    "        train_error[fold].append(cost_function(train_x, train_y,weights))\n",
    "        test_error[fold].append(cost_function(test_x, test_y,weights))\n",
    "        counter = 0\n",
    "        while counter < max_epochs:\n",
    "            counter += 1\n",
    "            #################stochastic gradient descent ######################\n",
    "            if method == 'SGD':\n",
    "                for i in range(len(train_y)):\n",
    "                    weights = update(train_x.iloc[i], train_y.iloc[i], learn_rate, method, weights)\n",
    "            else:\n",
    "                raise AttributeError('Specify gradient descent method as SGD')\n",
    "            ################# MSE for every fold  ############################\n",
    "            train_error[fold].append(cost_function(train_x, train_y, weights))\n",
    "            test_error[fold].append(cost_function(test_x, test_y, weights))\n",
    "        \n",
    "\n",
    "        scores_and_labels += [list(i) for i in zip(predict(test_x,weights), test_y)]\n",
    "    ################# After cross validation  ###################### \n",
    "    train_error = np.mean(np.array(train_error), axis=0)\n",
    "    test_error = np.mean(np.array(test_error), axis=0)\n",
    "    scores_and_labels = np.array(scores_and_labels)\n",
    "    \n",
    "    return train_error , test_error, scores_and_labels, weights\n",
    "\n",
    "def main():\n",
    "    data=pd.read_csv(\"spambase/spambase.data\", sep=\",\", header=None)\n",
    "    \n",
    "    # Split the dataset to X and y\n",
    "    y=data.iloc[:,-1]\n",
    "    X=data.iloc[:,:-1]\n",
    "    \n",
    "    #Normalize the data\n",
    "    for i in X:\n",
    "        X[i] = (X[i] - X[i].mean())/X[i].std()\n",
    "    # Add Bias term\n",
    "    X = pd.DataFrame(np.hstack((np.ones((X.shape[0],1)), X)))\n",
    "\n",
    "    \n",
    "    # Choose learning rate\n",
    "    learn_rate=[0.1,0.001,0.0001, 0.00001]\n",
    "    # Maximum number of steps\n",
    "    n_epochs=30\n",
    "    \n",
    "    # Plotting settings\n",
    "    epochs = [i for i in range(n_epochs+1)] #Number of epochs for plotting\n",
    "    training_style = ['b-', 'r-', 'g-', 'y-']\n",
    "    test_style = ['b--', 'r--', 'g--','y--']\n",
    "    \n",
    "    # Call cross_validate function\n",
    "    for i in range(0,len(learn_rate)):\n",
    "        (train_error, test_error, scores_and_labels, weights)=cross_validate(X, y,  method='SGD', learn_rate=learn_rate[i], max_epochs=n_epochs, k_fold=5)\n",
    "        plt.plot(epochs, train_error, training_style[i], epochs, test_error, test_style[i])\n",
    "    \n",
    "    \n",
    "    plt.title('Train and Validation Error of Logistic Regression' )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    # Make the plot pretty\n",
    "    blue = mpatches.Patch(color='blue', label='Learning rate = {}'.format(learn_rate[0]))\n",
    "    red = mpatches.Patch(color='red', label='Learning rate = {}'.format(learn_rate[1]))\n",
    "    green = mpatches.Patch(color='green', label='Learning rate = {}'.format(learn_rate[2]))\n",
    "    yellow = mpatches.Patch(color='yellow', label='Learning rate = {}'.format(learn_rate[3]))\n",
    "    solid = mlines.Line2D([], [], color='black', label='Training Error')\n",
    "    dashed = mlines.Line2D([], [], color='black', linestyle='--', label='Validation Error')\n",
    "    plt.legend(handles=[blue, red, green, yellow, solid, dashed ], numpoints=1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
